<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <title>CS 180 NeRF Project</title>
    <style>
      body {
        font-family: "Titillium Web", "HelveticaNeue-Light",
          "Helvetica Neue Light", "Helvetica Neue", Helvetica, "Lucida Grande",
          sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 60%;
        background-color: #f5f5f5;
      }

      h1,
      h2 {
        font-size: 35px;
        font-weight: 300;
        text-align: center;
      }

      h3 {
        font-size: 25px;
        font-weight: 300;
        text-align: center;
        line-height: 30px;
      }

      .code {
        font-size: 18px;
        font-family: "Courier New", Courier, monospace;
        background-color: #f0f0f0;
        padding: 2px 4px;
        border-radius: 3px;
      }

      .equation {
        text-align: center;
        background-color: #f9f9f9;
        padding: 10px;
        margin: 20px 0;
        font-family: "Courier New", Courier, monospace;
      }

      .description {
        margin-bottom: 30px;
        text-align: justify;
      }

      .centered-text {
        text-align: center;
      }

      .comparison-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 20px;
        margin-bottom: 30px;
        border-collapse: collapse;
      }

      .comparison-row {
        display: flex;
        flex-direction: column;
        align-items: center;
        text-align: center;
      }

      .comparison-image {
        width: 100%;
        height: auto;
        border: 1px solid #ddd;
        border-radius: 5px;
        margin-bottom: 5px;
      }

      .centered-container {
        text-align: center;
        margin-bottom: 30px;
      }

      .centered-image {
        display: block;
        margin-left: auto;
        margin-right: auto;
        width: 75%;
        height: auto;
        border: 1px solid #ddd;
        border-radius: 5px;
      }

      .image-title,
      .image-description {
        font-size: 16px;
        font-weight: 500;
        color: #333;
        margin: 10px 0;
      }

      .image-grid {
        display: grid;
        grid-template-columns: repeat(3, 1fr);
        grid-gap: 20px;
        margin-bottom: 30px;
      }

      .image-container {
        text-align: center;
      }

      .image-container img {
        width: 100%;
        height: auto;
        border: 1px solid #ddd;
        border-radius: 5px;
      }

      .column-title {
        grid-column: span 1;
        font-size: 30px;
        text-align: center;
        margin-bottom: 10px;
      }

      .table-title {
        grid-column: span 2;
        font-size: 18px;
        font-weight: bold;
        text-align: center;
        margin: 20px 0;
      }

      hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(
          to right,
          rgba(0, 0, 0, 0),
          rgba(0, 0, 0, 0.75),
          rgba(0, 0, 0, 0)
        );
      }
    </style>
  </head>
  <body>
    <header>
      <h1>CS 180: Neural Radiance Field (NeRF) Project</h1>
    </header>
    <main>
      <section class="description">
        <h2>Project Overview</h2>
        <p>
          This project explores the implementation of Neural Radiance Fields
          (NeRF) to represent 2D images and 3D scenes. The project is divided
          into two parts: fitting a neural field to a 2D image and creating a
          NeRF for multi-view 3D scenes. Techniques like sinusoidal positional
          encoding, ray sampling, and volume rendering are used to build and
          optimize these models.
        </p>
      </section>

      <section class="description">
        <h2>Part 1: Neural Field for 2D Image</h2>
        <p>
          In this part, a neural field is optimized to fit a 2D image. A
          multilayer perceptron (MLP) is used with sinusoidal positional
          encoding to learn the mapping from pixel coordinates to RGB values.
          The training process includes random pixel sampling and minimizing
          mean squared error loss.
        </p>
        <p>The architecture used is the one below:</p>
        <div class="centered-container">
          <img src="images/2d_structure.png" class="centered-image" />
        </div>
        <br />
        <p>The hyperparameters we used were:</p>
        <ul>
          <li>Number of layers: 4</li>
          <li>Channel size: 256</li>
          <li>Max frequency for the positional encoding: 10</li>
          <li>Learning rate: 0.01</li>
          <li>Batch Size: 10K</li>
          <li>Number of Epochs: 10</li>
        </ul>

        <div class="centered-container">
          <h3>Training Process Visualization</h3>

          <div class="image-grid">
            <div class="image-container">
              <img src="images/iteration_10.png" alt="iteration_10" />
              <p class="image-title">Iteration 10</p>
            </div>
            <div class="image-container">
              <img src="images/iteration_100.png" alt="iteration_100" />
              <p class="image-title">Iteration 100</p>
            </div>
            <div class="image-container">
              <img src="images/iteration_500.png" alt="iteration_500" />
              <p class="image-title">Iteration 500</p>
            </div>
            <div class="image-container">
              <img src="images/iteration_1000.png" alt="iteration_1000" />
              <p class="image-title">Iteration 800</p>
            </div>
            <div class="image-container">
              <img src="images/obama.png" alt="Obama" />
              <p class="image-title">Original Image</p>
            </div>
          </div>
          <p class="image-description">
            Visualization of the predicted image across training iterations.
          </p>
        </div>

        <div class="centered-container">
          <h4>PSNR Curve for Obama</h4>
          <img
            src="images/psnr_obama.png"
            alt="PSNR Curve for 2D Neural Field"
            class="centered-image"
          />
          <p class="image-description">
            Plot showing the PSNR across training iterations for the picture of
            Obama.
          </p>
        </div>

        <p>
          Here is the image of the fox with few iterations, many iterations and
          the original image.
        </p>

        <div class="image-grid">
          <div class="image-container">
            <img src="images/iteration_10 (1).png" alt="iteration_10" />
            <p class="image-title">Iteration 10</p>
          </div>
          <div class="image-container">
            <img src="images/iteration_50 (1).png" alt="iteration_100" />
            <p class="image-title">Iteration 100</p>
          </div>
          <div class="image-container">
            <img src="images/iteration_100 (1).png" alt="iteration_500" />
            <p class="image-title">Iteration 100</p>
          </div>
          <div class="image-container">
            <img src="images/iteration_500 (1).png" alt="iteration_1000" />
            <p class="image-title">Iteration 500</p>
          </div>
          <div class="image-container">
            <img src="images/animal.jpg" alt="Animal" />
            <p class="image-title">Original Image</p>
          </div>
        </div>

        <div class="centered-container">
          <h4>PSNR Curve for the fox</h4>
          <img
            src="images/psnr_fox.png"
            alt="PSNR Curve for 2D Neural Field"
            class="centered-image"
          />
          <p class="image-description">
            Plot showing the PSNR across training iterations for the Fox.
          </p>
        </div>

        <div class="image-grid">
          <div class="image-container">
            <img src="images/2d_input_image1.png" alt="Input Image 1" />
            <p class="image-title">Input Image 1</p>
          </div>
          <div class="image-container">
            <img
              src="images/2d_output_image1.png"
              alt="Reconstructed Image 1"
            />
            <p class="image-title">Reconstructed Image 1</p>
          </div>
          <div class="image-container">
            <img
              src="images/2d_output_image2.png"
              alt="Reconstructed Image 2"
            />
            <p class="image-title">Reconstructed Image 2</p>
          </div>
        </div>
      </section>

      <section class="description">
        <h2>Part 2: Neural Radiance Field for 3D Scenes</h2>
        <h2>Part 2.1: Create Rays from Cameras</h2>

        <h3>Camera to World Coordinate Conversion</h3>
        <p>
          We implemented the camera-to-world coordinate transformation by
          defining a function <code>x_w = transform(c2w, x_c)</code>, which
          converts points from the camera space to the world space. The inverse
          operation was also implemented to verify the correctness of the
          transformation. We used either NumPy or PyTorch for matrix
          multiplication, ensuring support for batched operations to handle
          multiple points efficiently.
        </p>

        <h3>Pixel to Camera Coordinate Conversion</h3>
        <p>
          To map pixel coordinates <code>(u, v)</code> to camera coordinates
          <code>(x_c, y_c, z_c)</code>, we defined a function
          <code>x_c = pixel_to_camera(K, uv, s)</code>, where <code>K</code> is
          the camera intrinsic matrix and <code>s</code> represents depth. This
          function was implemented to invert the projection process defined by
          the intrinsic matrix. We also added batch processing support for
          scalability during rendering.
        </p>

        <h3>Pixel to Ray Conversion</h3>
        <p>
          For converting pixel coordinates to rays, we implemented a function
          <code>ray_o, ray_d = pixel_to_ray(K, c2w, uv)</code>, where
          <code>ray_o</code> is the ray origin and <code>ray_d</code> is the
          normalized ray direction. This function leverages the earlier
          transformations and computes the ray origin as the camera position and
          the ray direction using normalized vector math. The implementation
          supports batched coordinates for processing multiple rays
          simultaneously.
        </p>

        <h2>Part 2.2: Sampling</h2>

        <h3>Sampling Rays from Images</h3>
        <p>
          We extended the random sampling approach from Part 1 to include
          multi-view images. A dataloader was implemented to handle multiple
          images and generate rays uniformly or with per-image sampling. The
          pixel coordinates were converted to rays using the functions from Part
          2.1, and each ray was associated with the corresponding pixel color.
        </p>

        <h3>Sampling Points along Rays</h3>
        <p>
          To sample 3D points along each ray, we defined <code>t</code> as a
          uniform range using <code>np.linspace(near, far, n_samples)</code> and
          computed each 3D point as <code>x = R_o + R_d * t</code>. To introduce
          randomness and avoid overfitting, we added a perturbation to
          <code>t</code> during training, ensuring better generalization. This
          step was crucial for generating accurate volume rendering results.
        </p>

        <h2>Part 2.3: Putting the Dataloader Together</h2>
        <p>
          We created a custom dataloader that processes pixel coordinates from
          multi-view images, converts them into rays, and samples 3D points
          along each ray. This dataloader returns the ray origin, ray direction,
          sampled points, and pixel colors. To verify correctness, we visualized
          a subset of rays and sampled points to ensure they matched the
          expected camera frustums.
        </p>

        <h2>Part 2.4: Neural Radiance Field</h2>

        <h3>Network Architecture</h3>
        <p>
          We modified the neural network from Part 1 to handle 3D inputs instead
          of 2D. The network now predicts both the density and RGB color for
          each 3D point. Positional encoding was used for the 3D input
          coordinates and ray directions, with a reduced frequency for ray
          direction encoding. We made the MLP deeper and injected the positional
          encoding features of the input at intermediate layers to improve
          gradient flow and model capacity.
        </p>
        <p>
          This part extends the neural field to 3D scenes using multi-view
          images. Rays are sampled from the images, and a volume rendering
          equation is applied to integrate densities and colors along the rays
          to generate pixel colors. Below is a diagram of the structure for the
          network implemented.
        </p>

        <div class="centered-container">
          <img src="images/structure.png" class="centered-image" />
        </div>

        <div class="centered-container">
          <h3>Camera and Ray Visualization</h3>
          <p>
            Below is the image of how the rays travel from the cameras through
            the images. The points are added to show how the rays sample points
            randomly on the rays and they are perturbed which we can see from
            the fact that the points are not spread out uniformly. The first
            image is a picture of rays from all cameras. The second image is
            rays from only one camera.
          </p>
          <img
            src="images/rays_and_samples.png"
            alt="Cameras and Rays"
            class="centered-image"
          />
          <img
            src="images/rays_and_samples_one_camera.png"
            alt="Cameras and Rays from One Camera"
            class="centered-image"
          />
          <p class="image-description">
            Visualization of sampled rays and camera frustums.
          </p>
        </div>

        <div class="centered-container">
          <h3>Volume Rendering Results</h3>
          <p class="image-description">
            Progression of the rendered images during training. The image starts
            out blurry and becomes sharper as the model learns the scene.
          </p>

          <div class="image-grid">
            <div class="image-container">
              <img src="images/e0b500.png" alt="e0b500" />
              <p class="image-title">Epoch 0 Batch 500</p>
            </div>
            <div class="image-container">
              <img src="images/e0b1000.png" alt="e0b1000" />
              <p class="image-title">Epoch 0 Batch 1000</p>
            </div>
            <div class="image-container">
              <img src="images/e1b500.png" alt="e1b500" />
              <p class="image-title">Epoch 1 Batch 500</p>
            </div>
            <div class="image-container">
              <img src="images/e1b1000.png" alt="e1b1000" />
              <p class="image-title">Epoch 1 Batch 1000</p>
            </div>
            <div class="image-container">
              <img src="images/e2b500.png" alt="e2b500" />
              <p class="image-title">Epoch 2 Batch 500</p>
            </div>
            <div class="image-container">
              <img src="images/e2b1000.png" alt="e2b1000" />
              <p class="image-title">Epoch 2 Batch 1000</p>
            </div>
          </div>
        </div>

        <p>
          Here is the video of the final result of the 3D scene from both low
          resolution and high resolution. The high resolution had the following
          hyperparameters:
        </p>
        <ul>
          <li>Number of positional encoding frequencies (L_x): 10</li>
          <li>Number of directional encoding frequencies (L_r_d): 4</li>
          <li>Hidden layer dimension: 256</li>
          <li>RGB output dimension: 3</li>
          <li>Density output dimension: 1</li>
          <li>Number of hidden layers: 8</li>
        </ul>

        <div class="comparison-container">
          <div class="comparison-row">
            <img
              src="images/nerf_render_low.gif"
              alt="Low Resolution"
              class="comparison-image"
            />
            <p class="image-description">Low Resolution</p>
          </div>
          <div class="comparison-row">
            <img
              src="images/nerf_render_high.gif"
              alt="High Resolution"
              class="comparison-image"
            />
            <p class="image-description">High Resolution</p>
          </div>
        </div>

        <div class="centered-container">
          <h3>PSNR Curve</h3>
          <p>
            As we can see from the plot below, the curve is rapidly increasing
            in the begninning and slowly converging to a high PSNR value of
            around 23-24 PSNR. Our final PSNR value was 23.6.
          </p>
          <img
            src="images/psnr_high.png"
            alt="PSNR Curve for 3D Scene"
            class="centered-image"
          />
          <p class="image-description">
            Plot showing the PSNR for validation images during training.
          </p>
        </div>
      </section>

      <section class="description">
        <h2>Bells and Whistles</h2>
        <p>
          For the B&W Portion, we added a blue background to the video by
          changing the volume function. The way we did this was by setting a
          threshold where if the transmittance was below a certain value, we
          would set the color to blue. This however, causes some aliasing issues
          as we can see which could have been fixed by implementing a solution
          where you add a color inverse proporsionally to the transmittance.
          Below is a video of the final result.
        </p>

        <div class="image-container">
          <img
            src="images/nerf_render_with_color.gif"
            alt="Custom Background Rendering"
            style="width: 50%; height: auto"
          />
          <p class="image-title">Custom Background Rendering</p>
        </div>
      </section>

      <section class="description">
        <h2>Acknowledgements</h2>
        <p>
          This project is part of CS 180 at UC Berkeley. Starter code and
          datasets were provided by course staff.
        </p>
      </section>
    </main>
  </body>
</html>
